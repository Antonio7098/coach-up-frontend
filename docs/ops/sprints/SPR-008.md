# Sprint SPR-008 — Decoupled Session Summary Endpoint

Short: Create a dedicated conversation summary service (decoupled from assessments) with its own storage, background cadence, and manual trigger.

## Meta
- Sprint ID: SPR-008
- Status: planned
- Start date: <YYYY-MM-DD>
- End date: <YYYY-MM-DD>
- Links: [Overview](./overview.md) · [Technical Overview](../../planning/technical-overview.md)

## Objectives (Tick when achieved)
- [x] Add POST /api/v1/session-summary (generate) — standalone, not tied to assessments
- [x] Read path (GET /api/v1/session-summary) — returns latest conversation summary from Convex
- [x] Backend cadence: generate every N turns or S seconds (env), tracked per session (ingest‑driven)

## Planned Tasks
- [x] Design payload and contracts — <owner> (0.5d)
  - Request (v1): { sessionId: string; sinceTs?: number; prevSummary?: string; messages?: Array<{ role: "user"|"assistant"; content: string }>; tokenBudget?: number }
  - Response (sync v1): { status: "completed"; summary: { sessionId: string; version: number; updatedAt: number } }
  - Notes:
    - Back-compat: also accept `recentMessages` for messages array.
    - Future async: { status: "queued"|"processing"; jobId: string } then GET returns latest when ready.
- [x] Implement server route (Node/Next API) — <owner> (1d)
  - [x] Server-fetch interactions since last summary (optional; `SUMMARY_FETCH_FROM_CONVEX=1`) with fallback to client-provided messages
  - [x] Summarize(previous summary + recent messages)
  - [x] Persist summary to Convex (new table)
- [x] Add backend cadence — <owner> (0.5d)
  - [x] Env: SUMMARY_GENERATE_ASSISTANT_EVERY_N, SUMMARY_GENERATE_SECONDS, SUMMARY_LOCK_MS (idempotency TBD)
  - [x] Per-session lock (Convex) to avoid duplicate triggers
- [x] UI minimal wiring — <owner> (0.5d)
  - [x] Button calls POST generate
  - [x] Panel displays latest summary (fresh panel)
- [x] Observability — <owner> (0.5d)
  - [x] Structured logs include requestId, sessionId; basic latency

## Scope
In scope
- Dedicated summary generation independent of assessments
- Background cadence triggering
- Manual override POST endpoint

Out of scope
- Assessment logic changes
- Complex token budgeting in v1 (basic cap only)

## Features in this Sprint
- [ ] FEAT-??? — Decoupled session summary (platform)

## Acceptance Criteria
- [x] POST /api/v1/session-summary persists a new summary row in `session_summaries`
- [x] GET /api/v1/session-summary returns latest from `session_summaries`
- [x] Cadence triggers auto-generation at configured thresholds (turns/seconds)
- [x] No assessment records are created or required for summary generation

## Risks & Mitigations
- Risk: Double-generation per turn · Mitigation: idempotency key per session window
- Risk: Large inputs · Mitigation: cap messages/time window; token budget

## Dependencies
- Convex access for summary persistence

## Technical Details
### Database Models (Convex)
- New table: `session_summaries`
  - sessionId: string (indexed)
  - version: number (monotonic per session)
  - text: string (summary body)
  - lastMessageTs: number (messages cutoff included)
  - createdAt: number; updatedAt: number
  - meta: { provider?: string; modelId?: string; tokenBudget?: number }
  - Indexes: by_session (sessionId, createdAt desc), by_createdAt
- New: `summary_state` per session for cadence/idempotency
  - sessionId: string (indexed)
  - turnsSince: number // assistant-completed turns since last summary
  - assistantMsgSince: number // assistant messages since last summary (for modulo-N trigger)
  - lastGeneratedAt: number // ms since epoch
  - lastVersion: number // last persisted summary version
  - lockUntil?: number // ms since epoch to prevent duplicate triggers
  - Index: by_session

### Algorithmic Details
- Triggering (ingest-driven):
  - After persisting an assistant message (finalized), call `functions/summaries_state:onAssistantMessage({ sessionId, lastKnownVersion })`.
  - That mutation increments counters and computes due if:
    - assistantMsgSince % SUMMARY_GENERATE_ASSISTANT_EVERY_N == 0 OR
    - now - lastGeneratedAt >= SUMMARY_GENERATE_SECONDS.
  - If due, it attempts to acquire a short lock: `lockUntil = now + SUMMARY_LOCK_MS` (compare-and-set) and returns `{ dueNow, locked, reason }`.
- Orchestration (UI API): if `locked`, fetch prev summary + recent messages, POST AI API `/api/v1/session-summary/generate` (cumulative prompt), persist to `session_summaries`, then call `functions/summaries_state:onGenerated({ sessionId, newVersion, generatedAt })` to reset counters and clear lock.
- Input to LLM: prior summary + messages since last cutoff; bounded by token/char cap. Prompt tuned to maintain a cumulative conversation‑flow log (topics, scenarios, techniques, open threads), deduplicated and non‑repetitive, prioritizing recent info under token limits.
- Targets: p95 generate < 2.5s.

### Prompts & Rubrics
- Prompt names: session_summary_v1 (Google; cumulative conversation flow)

## QA & Testing
- [x] Unit tests for POST and GET
- [x] AI API contract test: POST then GET returns new summary
- [ ] Cadence contract tests (modulo/time/concurrency) — deferred

## Observability & SLOs
- Targets: p95 generate < 2.5s
- Dashboards: add metrics to UI /metrics
- Metrics to add (UI API): `summaries_triggered_total{reason}`, `summary_generate_latency_ms`, `summary_age_ms`, `summary_locks_total{acquired|contended}`

## Issues & Deviations
- Early UI auto-cadence sometimes fired with zero messages; guarded to require content or existing summary.
- Panel showed truncated outputs in some cases; AI prompt adjusted to avoid dangling sections and raised token floor.

## Operational Hygiene
- [ ] .env.example updated (SUMMARY_GENERATE_ASSISTANT_EVERY_N, SUMMARY_GENERATE_SECONDS, SUMMARY_LOCK_MS, SUMMARY_IDEMPOTENCY_TTL_SECONDS, SUMMARY_FETCH_FROM_CONVEX)
- [x] Request ID propagated
- [ ] Rate limit + idempotency

### Proposed next steps
- Idempotency + rate limiting for POST `/api/v1/session-summary` and `/api/v1/interactions`
- Bounded server window and lightweight dedupe before LLM call
- Markdown rendering in Summary panel; highlight changes since last cutoff
- Retry/backoff around AI API with circuit-breaker metrics
- E2E contract tests for cadence (turn/time) and ingestion flow
- Prefetch summary during assistant TTS playback to mask latency
- Consolidate debug logs behind a toggle

## Documentation
- [x] API reference updated

## Next Steps & Polish
- [x] UI API composition (client-first; server-fetch optional)
  - [x] Client-first: use `prevSummary` + client-provided `messages` on the critical path
  - [x] Prefetch after assistant completes to warm the UI
  - [x] Server since‑cutoff fetch optional via `SUMMARY_FETCH_FROM_CONVEX=1` with fallback to client `messages`
  - [x] Compose LLM input as previous summary + messages since cutoff (bounded)
  - [x] Call AI API; persist to `session_summaries`; update `summary_state`
- [x] Coach-min integration
  - [x] Route Generate to the UI API POST (server-composed prompt)
  - [x] Remove UI auto-cadence; rely on server cadence and show server thresholds with local delta between fetches
  - [x] Add Server Transcript and Session State debug panels
  - [x] Persist interactions for mic and typed paths via `/api/v1/interactions`
  - [x] Add a visible "Test ingest" button to verify end-to-end ingestion
- [x] Robustness
  - [x] Never wipe summary: if AI returns empty text, reuse previous summary and keep prior cutoff
  - [x] Safer fallbacks when server interactions since-cutoff are empty
- [ ] Cumulative behavior polish
  - [ ] Enforce a bounded server window (e.g., last 20–40 messages or 10–15 minutes)
  - [ ] Simple dedupe/merge of repeated items before sending to LLM
- [ ] History UX
  - [ ] Render markdown in the panel for clearer bullets/headings
  - [ ] Keep local history; add a toggle to show raw length and tail preview for debug
- [ ] Config and docs
  - [x] Add cadence/envs to documentation (including `SUMMARY_FETCH_FROM_CONVEX`)
  - [ ] Expand README with cumulative flow policy (inputs, priority rules)
- [ ] Cleanup
  - [ ] Trim noisy logs; keep only high-signal debug toggles.

## LLM Prompt Integration & Debug Panel

### Goal
Incorporate the latest session summary into the LLM prompt so chat responses stay grounded in the cumulative conversation flow. Add a debug panel in Coach‑min to display the exact prompt used (behind a debug flag), passed through from the backend.

### Design
- Prompt structure (for chat generation routes):
  - System preamble: concise instruction set for style and safety.
  - Session summary context: latest cumulative summary (trimmed/bounded).
  - Recent messages: since‑cutoff slice, bounded by count/time and lightly deduped.
  - Task: the user’s current prompt.
- Bounds:
  - Max summary tokens (e.g., ~600–800 tokens; truncate tail if needed).
  - Since‑cutoff window: last 40 messages and/or last 10–15 minutes, whichever is smaller.
  - Light dedupe: hash or high‑similarity collapse of repeated content.

### Backend changes
- Chat generation endpoint(s):
  - Build full prompt: { system, summary, recentMessages, task }.
  - Expose a debug payload when `PROMPT_DEBUG=1` (or `X-Debug-Prompt: 1`):
    - Include `promptPreview` with safely truncated sections and lengths, or return via debug headers/body.
    - Never expose in production unless explicitly enabled.
- Summary POST orchestration (optional):
  - Include `promptPreview` for the summary generation request when `PROMPT_DEBUG=1`.

### UI changes (Coach‑min)
- New panel: “LLM Prompt (debug)”
  - Toggle button to show/hide.
  - Renders `promptPreview` sections with lengths and truncation hints.
  - Works for both: recent chat turns and summary generation POST.
- Respect a client flag (e.g., `NEXT_PUBLIC_PROMPT_DEBUG=1`) to display panel.

### Observability & Safety
- Truncate large sections; show character/token counts.
- Redact sensitive values when necessary.
- Gate via env/header; default off in production.

### Acceptance Criteria
- Chat requests include the latest summary in the prompt.
- With debugging enabled, the UI panel shows the constructed prompt preview for the last request.
- No prompt data is exposed when debug is disabled.

## Post-sprint
- [ ] KPIs reviewed
- [ ] Retrospective

## Change Log
- <YYYY-MM-DD> Created sprint page
-. 2025-09-01 UI API composition completed; server cadence enabled; ingestion wired; empty-text fallback implemented; docs updated
