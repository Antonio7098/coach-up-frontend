# Sprint SPR-008 — Decoupled Session Summary Endpoint

Short: Create a dedicated conversation summary service (decoupled from assessments) with its own storage, background cadence, and manual trigger.

## Meta
- Sprint ID: SPR-008
- Status: planned
- Start date: <YYYY-MM-DD>
- End date: <YYYY-MM-DD>
- Links: [Overview](./overview.md) · [Technical Overview](../../planning/technical-overview.md)

## Objectives (Tick when achieved)
- [x] Add POST /api/v1/session-summary (generate) — standalone, not tied to assessments
- [x] Read path (GET /api/v1/session-summary) — returns latest conversation summary from Convex
- [x] Backend cadence: generate every N turns or S seconds (env), tracked per session (ingest‑driven)

## Planned Tasks
- [x] Design payload and contracts — <owner> (0.5d)
  - Request (v1): { sessionId: string; sinceTs?: number; prevSummary?: string; messages?: Array<{ role: "user"|"assistant"; content: string }>; tokenBudget?: number }
  - Response (sync v1): { status: "completed"; summary: { sessionId: string; version: number; updatedAt: number } }
  - Notes:
    - Back-compat: also accept `recentMessages` for messages array.
    - Future async: { status: "queued"|"processing"; jobId: string } then GET returns latest when ready.
- [x] Implement server route (Node/Next API) — <owner> (1d)
  - [x] Server-fetch interactions since last summary (optional; `SUMMARY_FETCH_FROM_CONVEX=1`) with fallback to client-provided messages
  - [x] Summarize(previous summary + recent messages)
  - [x] Persist summary to Convex (new table)
- [x] Add backend cadence — <owner> (0.5d)
  - [x] Env: SUMMARY_GENERATE_ASSISTANT_EVERY_N, SUMMARY_GENERATE_SECONDS, SUMMARY_LOCK_MS (idempotency TBD)
  - [x] Per-session lock (Convex) to avoid duplicate triggers
- [x] UI minimal wiring — <owner> (0.5d)
  - [x] Button calls POST generate
  - [x] Panel displays latest summary (fresh panel)
- [x] Observability — <owner> (0.5d)
  - [x] Structured logs include requestId, sessionId; basic latency

## Scope
In scope
- Dedicated summary generation independent of assessments
- Background cadence triggering
- Manual override POST endpoint

Out of scope
- Assessment logic changes
- Complex token budgeting in v1 (basic cap only)

## Features in this Sprint
- [ ] FEAT-??? — Decoupled session summary (platform)

## Acceptance Criteria
- [x] POST /api/v1/session-summary persists a new summary row in `session_summaries`
- [x] GET /api/v1/session-summary returns latest from `session_summaries`
- [x] Cadence triggers auto-generation at configured thresholds (turns/seconds)
- [x] No assessment records are created or required for summary generation

## Risks & Mitigations
- Risk: Double-generation per turn · Mitigation: idempotency key per session window
- Risk: Large inputs · Mitigation: cap messages/time window; token budget

## Dependencies
- Convex access for summary persistence

## Technical Details
### Database Models (Convex)
- New table: `session_summaries`
  - sessionId: string (indexed)
  - version: number (monotonic per session)
  - text: string (summary body)
  - lastMessageTs: number (messages cutoff included)
  - createdAt: number; updatedAt: number
  - meta: { provider?: string; modelId?: string; tokenBudget?: number }
  - Indexes: by_session (sessionId, createdAt desc), by_createdAt
- New: `summary_state` per session for cadence/idempotency
  - sessionId: string (indexed)
  - turnsSince: number // assistant-completed turns since last summary
  - assistantMsgSince: number // assistant messages since last summary (for modulo-N trigger)
  - lastGeneratedAt: number // ms since epoch
  - lastVersion: number // last persisted summary version
  - lockUntil?: number // ms since epoch to prevent duplicate triggers
  - Index: by_session

### Algorithmic Details
- Triggering (ingest-driven):
  - After persisting an assistant message (finalized), call `functions/summaries_state:onAssistantMessage({ sessionId, lastKnownVersion })`.
  - That mutation increments counters and computes due if:
    - assistantMsgSince % SUMMARY_GENERATE_ASSISTANT_EVERY_N == 0 OR
    - now - lastGeneratedAt >= SUMMARY_GENERATE_SECONDS.
  - If due, it attempts to acquire a short lock: `lockUntil = now + SUMMARY_LOCK_MS` (compare-and-set) and returns `{ dueNow, locked, reason }`.
- Orchestration (UI API): if `locked`, fetch prev summary + recent messages, POST AI API `/api/v1/session-summary/generate` (cumulative prompt), persist to `session_summaries`, then call `functions/summaries_state:onGenerated({ sessionId, newVersion, generatedAt })` to reset counters and clear lock.
- Input to LLM: prior summary + messages since last cutoff; bounded by token/char cap. Prompt tuned to maintain a cumulative conversation‑flow log (topics, scenarios, techniques, open threads), deduplicated and non‑repetitive, prioritizing recent info under token limits.
- Targets: p95 generate < 2.5s.

### Prompts & Rubrics
- Prompt names: session_summary_v1 (Google; cumulative conversation flow)

## QA & Testing
- [x] Unit tests for POST and GET
- [x] AI API contract test: POST then GET returns new summary
- [ ] Cadence contract tests (modulo/time/concurrency) — deferred

### Detailed Testing Plan
#### Unit Tests
- Test POST `/api/v1/session-summary` endpoint: Validate payload parsing, Convex persistence (mocked), versioning, and error handling (e.g., invalid sessionId).
- Test GET `/api/v1/session-summary` endpoint: Mock Convex queries to verify latest summary retrieval by sessionId, handling empty results.
- Test cadence logic: Mock `summary_state` mutations in Convex functions (e.g., `functions/summaries_state:onAssistantMessage`); simulate turn/seconds thresholds; assert lock acquisition via compare-and-set.
- Test prompt composition: Assert LLM input builds correctly (previous summary + messages); handle fallbacks (server-fetch failures) and token caps (600-800 tokens).
- Test UI wiring: Verify button triggers POST, panel displays summary; mock debug panel for prompt previews.
- Tools: Jest/Mocha with mocked Convex API and AI service; focus on `coach-up-ai-api` routes.
- Coverage: 70-80% on endpoints and functions; include edge cases like empty AI responses.

#### Integration Tests
- Test full cadence flow: Simulate assistant message persistence → trigger cadence → acquire lock → POST summary → persist to `session_summaries` → reset counters.
- Test server-fetch integration: Enable `SUMMARY_FETCH_FROM_CONVEX=1`; verify interactions fetched since cutoff; fallback to client messages.
- Test prompt integration: Chat routes include session summary; assert grounding in responses; enable `PROMPT_DEBUG=1` for debug headers.
- Edge Cases: Concurrent requests (lock contention); large inputs (token budget enforcement); server interactions empty (safe fallbacks).
- Tools: Supertest for API routes; Convex dev server for mutations.

#### E2E Tests (Playwright)
- Happy Path: User loads Coach-min → triggers manual summary generation → panel updates with new version; verify versioning and cutoff.
- Scenarios: Simulate auto-cadence (mock turns/seconds) → check background generation; test debug panel (enable flag → view prompt preview).
- Coach-min Integration: End-to-end ingestion (mic/typed paths) → summary persists; verify "Test ingest" button.
- Tools: Playwright for browser automation; integrate with staging env.

#### Smoke Tests
- Quick sanity: Deploy → POST summary → GET returns valid data; no crashes on cadence triggers or empty responses.
- Tools: k6 for load/SSE; manual API pings via curl.

## Observability & SLOs
- Targets: p95 generate < 2.5s
- Dashboards: add metrics to UI /metrics
- Metrics to add (UI API): `summaries_triggered_total{reason}`, `summary_generate_latency_ms`, `summary_age_ms`, `summary_locks_total{acquired|contended}`

### Detailed Monitoring Plan (Prometheus/Grafana)
#### Metrics to Collect (via Prometheus)
- Latency & Performance: `summary_generate_latency_ms` (histogram); `summaries_triggered_total{reason}` (counter: manual/auto); error rates for POST/GET endpoints.
- Cadence & Locks: `summary_locks_total{acquired|contended}` (gauge); `summary_age_ms` (time since last generation).
- Token Usage: Custom metric for prompt tokens in summaries/chats; budget utilization (e.g., tokens used / cap).
- Debug/Integration: `prompt_debug_requests_total` (counter for debug panel usage); `server_fetch_fallbacks_total` (for `SUMMARY_FETCH_FROM_CONVEX=1` failures).
- Instrument code in `coach-up-ai-api` and Convex functions; expose via `/metrics` endpoint.

#### Grafana Dashboards
- Latency Dashboard: Trends for generate latency, trigger rates by reason, error rates; p95 alerts (>2.5s).
- Cadence Health: Lock acquisition/contention charts; summary freshness (age_ms) with thresholds.
- Token Budget: Utilization graphs for prompts; alerts on overruns.
- Debug Panel: Usage stats; ensure no production exposure.
- Setup: Scrape Prometheus from app/Convex; build dashboards from defined metrics; integrate with existing UI /metrics.

#### Alerts & SLOs
- Alerts: Latency >2.5s; high lock contention; token budget >90%; debug flag enabled in prod.
- SLOs: 99% uptime for endpoints; p95 latency <2.5s for generations.
- Logs: Structured with requestId, sessionId, tokens, cost, latency; redact sensitive prompt data.

## Issues & Deviations
- Early UI auto-cadence sometimes fired with zero messages; guarded to require content or existing summary.
- Panel showed truncated outputs in some cases; AI prompt adjusted to avoid dangling sections and raised token floor.

## Operational Hygiene
- [ ] .env.example updated (SUMMARY_GENERATE_ASSISTANT_EVERY_N, SUMMARY_GENERATE_SECONDS, SUMMARY_LOCK_MS, SUMMARY_IDEMPOTENCY_TTL_SECONDS, SUMMARY_FETCH_FROM_CONVEX)
- [x] Request ID propagated
- [ ] Rate limit + idempotency

### Proposed next steps
- Idempotency + rate limiting for POST `/api/v1/session-summary` and `/api/v1/interactions`
- Bounded server window and lightweight dedupe before LLM call
- Markdown rendering in Summary panel; highlight changes since last cutoff
- Retry/backoff around AI API with circuit-breaker metrics
- E2E contract tests for cadence (turn/time) and ingestion flow
- Prefetch summary during assistant TTS playback to mask latency
- Consolidate debug logs behind a toggle

## Documentation
- [x] API reference updated

## Next Steps & Polish
- [x] UI API composition (client-first; server-fetch optional)
  - [x] Client-first: use `prevSummary` + client-provided `messages` on the critical path
  - [x] Prefetch after assistant completes to warm the UI
  - [x] Server since‑cutoff fetch optional via `SUMMARY_FETCH_FROM_CONVEX=1` with fallback to client `messages`
  - [x] Compose LLM input as previous summary + messages since cutoff (bounded)
  - [x] Call AI API; persist to `session_summaries`; update `summary_state`
- [x] Coach-min integration
  - [x] Route Generate to the UI API POST (server-composed prompt)
  - [x] Remove UI auto-cadence; rely on server cadence and show server thresholds with local delta between fetches
  - [x] Add Server Transcript and Session State debug panels
  - [x] Persist interactions for mic and typed paths via `/api/v1/interactions`
  - [x] Add a visible "Test ingest" button to verify end-to-end ingestion
- [x] Robustness
  - [x] Never wipe summary: if AI returns empty text, reuse previous summary and keep prior cutoff
  - [x] Safer fallbacks when server interactions since-cutoff are empty
- [ ] Cumulative behavior polish
  - [ ] Enforce a bounded server window (e.g., last 20–40 messages or 10–15 minutes)
  - [ ] Simple dedupe/merge of repeated items before sending to LLM
- [ ] History UX
  - [ ] Render markdown in the panel for clearer bullets/headings
  - [ ] Keep local history; add a toggle to show raw length and tail preview for debug
- [ ] Config and docs
  - [x] Add cadence/envs to documentation (including `SUMMARY_FETCH_FROM_CONVEX`)
  - [ ] Expand README with cumulative flow policy (inputs, priority rules)
- [ ] Cleanup
  - [ ] Trim noisy logs; keep only high-signal debug toggles.

## LLM Prompt Integration & Debug Panel

### Goal
Incorporate the latest session summary into the LLM prompt so chat responses stay grounded in the cumulative conversation flow. Add a debug panel in Coach‑min to display the exact prompt used (behind a debug flag), passed through from the backend.

### Design
- Prompt structure (for chat generation routes):
  - System preamble: concise instruction set for style and safety.
  - Session summary context: latest cumulative summary (trimmed/bounded).
  - Recent messages: since‑cutoff slice, bounded by count/time and lightly deduped.
  - Task: the user’s current prompt.
- Bounds:
  - Max summary tokens (e.g., ~600–800 tokens; truncate tail if needed).
  - Since‑cutoff window: last 40 messages and/or last 10–15 minutes, whichever is smaller.
  - Light dedupe: hash or high‑similarity collapse of repeated content.

### Backend changes
- Chat generation endpoint(s):
  - Build full prompt: { system, summary, recentMessages, task }.
  - Expose a debug payload when `PROMPT_DEBUG=1` (or `X-Debug-Prompt: 1`):
    - Include `promptPreview` with safely truncated sections and lengths, or return via debug headers/body.
    - Never expose in production unless explicitly enabled.
- Summary POST orchestration (optional):
  - Include `promptPreview` for the summary generation request when `PROMPT_DEBUG=1`.

### UI changes (Coach‑min)
- New panel: “LLM Prompt (debug)”
  - Toggle button to show/hide.
  - Renders `promptPreview` sections with lengths and truncation hints.
  - Works for both: recent chat turns and summary generation POST.
- Respect a client flag (e.g., `NEXT_PUBLIC_PROMPT_DEBUG=1`) to display panel.

### Observability & Safety
- Truncate large sections; show character/token counts.
- Redact sensitive values when necessary.
- Gate via env/header; default off in production.

### Acceptance Criteria
- Chat requests include the latest summary in the prompt.
- With debugging enabled, the UI panel shows the constructed prompt preview for the last request.
- No prompt data is exposed when debug is disabled.

## Post-sprint
- [ ] KPIs reviewed
- [ ] Retrospective

## Change Log
- <YYYY-MM-DD> Created sprint page
-. 2025-09-01 UI API composition completed; server cadence enabled; ingestion wired; empty-text fallback implemented; docs updated
