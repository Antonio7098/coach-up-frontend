# Sprint SPR-005 — Real LLM Integration & Provider Switching

Implement real LLM calls in the AI API with a swappable provider layer and first-class mocking. Support Google and OpenRouter providers with three model tiers: chat (normal), interaction classification (ultralight), and assessment (heavier). Land thorough tests, monitoring, and docs while preserving existing flows.

## Meta
- Sprint ID: SPR-005
- Status: active
- Start date: 2025-08-23
- End date: 2025-09-06
- Links: [Overview](./overview.md) · [PRD](../../planning/prd.md) · [Technical Overview](../../planning/technical-overview.md) · [Monitoring](../../ops/monitoring.md) · [Benchmarking](../../ops/benchmarking.md) · [Features CSV](../features.csv) · [AI API Reference](../../../../coach-up-ai-api/docs/api/ai/reference.md)

## Objectives (Tick when achieved) 
- [ ] Real LLM provider abstraction with Google and OpenRouter backends, plus mock
- [ ] Chat SSE streams tokens from real provider model (fallback to mock)
- [ ] Interaction classification calls ultralight model with heuristic fallback
- [ ] Real LLM calls are used for chat, classifier, and assessor when enabled (mock remains default in CI)
- [ ] Assessment job outputs simple, skill-aligned results (per tracked skill: level + met/unmet criteria + brief feedback) — no rubric v1
- [ ] Provider/model switching via env flags without code changes
- [ ] Tests: unit, integration, e2e green; deterministic mock for CI
- [ ] Metrics/logging include provider, modelId, tokens, cost, latency; dashboards updated
- [ ] API docs and examples updated; OpenAPI generated and linted

## Planned Tasks
- [ ] Add provider abstraction in AI API — owner: backend (<5d)
  - [ ] Create `app/providers/base.py` with `ChatClient`, `ClassifierClient`, `AssessClient` interfaces
  - [ ] Implement `app/providers/google.py` (Google Generative AI) and `app/providers/openrouter.py`
  - [ ] Implement `app/providers/mock.py` (deterministic mock, supports stream)
  - [ ] Add `app/providers/factory.py` to instantiate based on env (`AI_PROVIDER`, per-role overrides)
  - [ ] Wire requestId propagation and user agent; add timeouts/retries

- [ ] Chat SSE uses real model — owner: backend (<2d)
  - [ ] Replace `_token_stream()` in `app/main.py` with provider stream when `AI_CHAT_ENABLED=1`
  - [ ] Emit TTFT and total latency metrics; include provider/model in logs and labels
  - [ ] Fallback to mock stream when provider disabled or error

- [ ] Interaction classification LLM — owner: backend (<2d)
  - [ ] Replace `_classify_message_llm_stub` with provider call when `AI_CLASSIFIER_ENABLED=1`
  - [ ] Keep heuristic fallback for low/abstain and errors; preserve thresholds
  - [ ] Add prompts and minimal schema; log decision, confidence, provider/model

- [ ] Assessment job LLM — owner: backend (<3d)
  - [ ] In `_run_assessment_job`, when enabled, call assess provider to compute per-skill level and feedback from transcript slice
  - [ ] Preserve deterministic mock outputs to keep CI stable (no dependency on real LLMs)
  - [ ] Include transcript slice context and tracked skill criteria in prompt; cap tokens/latency
  - [ ] Fan-out: spawn one assessor LLM call per tracked skill for each queued interaction; pass the skill definition and criteria to the prompt
  - [ ] Aggregation: build one group result with `skillAssessments[]` in stable order; aggregate tokens/cost; include per-skill errors for partial failures; enforce per-call timeouts and a group deadline

- [ ] Assessment model migration (plan only) — owner: backend (<1d)
  - [ ] Deprecate rubric v1 categories/scores; define minimal skill-aligned output shape:
        `{ skillAssessments: [{ trackedSkillIdHash?, level, metCriteria[], unmetCriteria[], feedback[] }], meta{...} }`
  - [ ] Update AI API docs to describe new shape and note deprecation; maintain temporary backward-compat notes where needed
  - [ ] Inventory and plan updates for all tests touching rubric (goldens, shape assertions, persistence contract)

- [ ] Tracked skills integration — owner: backend (<2d)
  - [ ] Fetch tracked skills for the session (or from `x-tracked-skill-id(s)`) via Core/UI API or Convex
  - [ ] Env: `SKILLS_API_URL`, `SKILLS_API_SECRET` (service-to-service); support `X-Request-Id` propagation
  - [ ] Support multiple skills; pass skill definitions to assessor; log `skillCount` and hashed IDs
  - [ ] Backwards-compatible path when no skills present (assess general communication)

- [ ] Configuration & flags — owner: backend (<1d)
  - [ ] Env vars: `AI_PROVIDER`, `AI_CHAT_MODEL`, `AI_CLASSIFIER_MODEL`, `AI_ASSESS_MODEL`
  - [ ] Provider keys: `GOOGLE_API_KEY`, `OPENROUTER_API_KEY`
  - [ ] Per-role provider overrides: `AI_PROVIDER_CHAT`, `AI_PROVIDER_CLASSIFIER`, `AI_PROVIDER_ASSESS`
  - [ ] Feature flags: `AI_CHAT_ENABLED`, `AI_CLASSIFIER_ENABLED`, `AI_ASSESS_ENABLED`; default off for CI
  - [ ] Update `.env.example` and docs
  - [ ] Default models:
    - Google: chat=`gemini-2.5-flash`, classifier=`gemini-2.5-flash-lite`, assess=`gemini-2.5-pro`
    - OpenRouter: chat=`google/gemini-2.5-flash`, classifier=`google/gemini-2.5-flash-lite`, assess=`google/gemini-2.5-pro`
  - [ ] Skills fetch config: `SKILLS_API_URL`, `SKILLS_API_SECRET`, optional `SKILLS_PROVIDER` (convex|core)

- [ ] Observability — owner: infra (<2d)
  - [ ] Add Prometheus counters/histograms: tokens, cost_usd, ttft_seconds, duration_seconds by provider/model/route
  - [ ] Log fields: `requestId`, `provider`, `modelId`, `tokensPrompt`, `tokensOutput`, `costUsd`, `latencyMs`
  - [ ] Update Grafana dashboards: chat SSE, assessments latency, queue depth, provider/model breakdown
  - [ ] Define alert rules and thresholds (non-enforcing cost alerts; enforcement tracked in FEAT-022):
    - chat_ttft_p95 > 1.2s for 5m (warn); > 1.5s for 10m (crit)
    - assessment_completion_p95 > 8s for 10m (warn); > 12s for 10m (crit)
    - http_5xx_rate > 1% for 5m (warn); > 5% for 5m (crit)
    - queue_depth > 100 for 10m (warn); > 500 for 10m (crit)
    - cost_usd_daily > 80% of `COST_BUDGET_USD_DAILY` (warn); > 100% (crit)
  - [ ] Alert routing configured (Slack channel + on-call escalation for critical alerts)

- [ ] Testing — owner: QA (<3d)
  - [ ] Unit tests: provider factory selection, env parsing, chat stream adapter, classifier fallback, assess mapping
  - [ ] Integration tests: chat SSE happy path (mock provider), classifier decisions, assessment end-to-end (mock)
  - [ ] Contract tests: request/response shapes unchanged on `/chat/stream`, `/messages/ingest`, `/assessments/*`
  - [ ] E2E (Playwright) smoke uses mock provider; add toggle for real provider manual runs
  - [ ] Load/SSE k6 smoke for chat stream

- [ ] Documentation & examples — owner: docs (<1d)
  - [ ] Update `docs/api/ai/reference.md` with provider/model switching and headers
  - [ ] Regenerate `/openapi.json`; add example curl + TS for both providers
  - [ ] Add `examples/request_id_middleware.py` mention for request tracing

## Scope
In scope
- Real LLM integration for chat, classification, and assessment via provider layer
- Provider switching and mocking via env flags
- Metrics/logging and dashboards for new paths
- Tests (unit, integration, e2e) and API docs updates

Out of scope
- Auth (JWT verification) beyond current stubs
- Cost alarms/budget enforcement (tracked in FEAT-022)
- STT/TTS integrations

## Features in this Sprint
- [ ] FEAT-004 — Provider Abstraction Layer (ai)
- [ ] FEAT-011 — Multi-turn Assessment Job v1 (ai)
- [ ] FEAT-019 — Monitoring Baseline (infra)
- [ ] FEAT-028 — Skill-aligned Assessments v0 (ai)
- [ ] FEAT-030 — LLM Boundary Classifier (ai)

## Acceptance Criteria
- [ ] `AI_PROVIDER` and per-role overrides select Google, OpenRouter, or mock without code changes
- [ ] `/chat/stream` streams tokens from the configured chat model; TTFT logged and < 1.2s p95 with mock
- [ ] `/messages/ingest` uses LLM classification when enabled; retains heuristic fallback; decisions logged with confidence
- [ ] `/assessments/run` + `/assessments/{sessionId}` produce simple skill-aligned results (per tracked skill: level + met/unmet criteria + brief feedback); heavy model path guarded by flag
- [ ] For N tracked skills in an interaction, N assessor LLM calls are executed and N items are returned in `skillAssessments[]`
- [ ] Aggregation returns a single group payload with stable ordering; partial failures are included as per-skill errors; `meta` aggregates tokens/cost and latency
- [ ] Assessor fetches tracked skills and evaluates against them; summarizer outputs highlights/recommendations; orchestrator composes
- [ ] Metrics exported with provider/model labels; dashboards updated and linked
- [ ] Tests: all unit/integration/e2e pass with mock provider in CI; deterministic goldens preserved (updated to new output shape)
- [ ] Docs updated; OpenAPI refreshed; examples added
- [ ] Manual local verification with Google and OpenRouter providers using real keys

## Risks & Mitigations
- Risk: Provider quotas/latency spikes · Mitigation: default mock in CI; timeouts/retries; provider failover
- Risk: Non-deterministic tests due to real LLMs · Mitigation: mock by default; snapshot tests only on mock
- Risk: SSE proxy timeouts · Mitigation: heartbeat every 15s; backpressure handling; timeouts
- Risk: Cost overruns · Mitigation: cost metrics per request; manual toggle for real-provider E2E only

## Dependencies
- Depends on features/sprints: FEAT-019, FEAT-028, FEAT-011
- External: Google Generative AI API key, OpenRouter API key; optional SQS for assessments

## Technical Details
### Database Models
- Collections/Tables impacted: none (in-memory + optional HTTP persist remains)
- New/changed fields: N/A
- Constraints/validation: N/A
- Indexes (read/write paths): N/A
- Migrations: N/A — Backfill plan: N/A
- Data retention: raw logs 30d; SSE logs sampled

### Algorithmic Details
- Approach: LLM-first with heuristic fallback (minimal-edit); deterministic mock for CI
- Skill-aligned output: v0 (per tracked skill: assign level and identify met/unmet criteria; produce brief feedback)
- Fan-out: for each queued interaction with N tracked skills, spawn N independent assessor LLM calls; each call receives the specific skill definition
- Aggregation: collect per-skill results into `skillAssessments[]` with stable ordering; aggregate tokens/cost and latency; include per-skill errors on timeouts/failures; enforce per-call timeouts and a group deadline
- Latency targets: TTFT < 1.2s (mock); full-turn < 2.5s (mock)

### Prompts & Scoring
- Prompt names/links: chat.system, classifier.boundary, assess.skill-v0
- Leveling criteria: per tracked skill; identify met/unmet criteria and provide brief feedback

## QA & Testing
- [ ] Unit tests updated/added (providers, factory, adapters)
- [ ] E2E happy path green (Playwright) with mock provider
- [ ] Assert N tracked skills -> N assessor invocations and mapping to `skillAssessments[]`; test partial failure path (timeouts/errors)
- [ ] Load/SSE smoke test passes (k6)
- [ ] Contract tests for API routes (`/chat/stream`, `/messages/ingest`, `/assessments/*`)

## Observability & SLOs
Targets (see Technical Overview §10)
- Realtime chat p95 TTFT < 1.2s (mock); full-turn < 2.5s (mock)
- Assessment p95 completion < 8s (mock/in-memory)
- Alert thresholds
  - Chat TTFT p95 > 1.2s 5m warn; > 1.5s 10m crit
  - Assessment completion p95 > 8s 10m warn; > 12s 10m crit
  - HTTP 5xx rate > 1% 5m warn; > 5% 5m crit
  - Queue depth > 100 10m warn; > 500 10m crit
  - Daily cost spend > 80% budget warn; > 100% budget crit (uses `COST_BUDGET_USD_DAILY`)

Checkpoints
- [ ] Dashboards exist/updated (links)
- [ ] Alerts configured (links)
- [ ] Structured logs include requestId, provider, modelId, tokens, cost, latency
- [ ] Alert thresholds documented and rules enabled in Grafana/Prometheus
- [ ] Ops runbook linked (monitoring, alert response, dashboards)
- [ ] On-call rotation documented with escalation path

## Operational Hygiene
- [ ] CI checks green (API Docs workflow, tests/linting)
- [ ] Branch protection respected (PR + review)
- [ ] Pre-commit hooks executed (lint/format/type checks)
- [ ] .env.example updated if new env vars added
- [ ] Request ID propagated end-to-end for changed paths
- [ ] Logs include: requestId, route, userId (if available), provider, modelId, tokens, cost, latency
- [ ] Rate limiting and idempotency considered for new/changed endpoints
- [ ] On-call schedule maintained; escalation policy verified
- [ ] Ops runbook created (monitoring, alert thresholds, troubleshooting) and referenced
- [ ] Incident template used; postmortems recorded and linked

## Documentation
- [ ] API reference updated for endpoints touched (Core & AI)
- [ ] OpenAPI spec updated and linted
- [ ] Examples added/verified (curl + TypeScript)
- [ ] Cross-links updated (PRD/Technical Overview)
- [ ] Ops runbook added and linked (dashboards, alerts, playbooks)

## Post-sprint
- [ ] KPIs reviewed; compare to targets
- [ ] Retrospective completed; action items filed
- [ ] Docs updated (PRD/Tech Overview/Runbooks)

## Change Log
- 2025-08-23 Created sprint page