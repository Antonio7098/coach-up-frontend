# Sprint SPR-005 — Real LLM Integration & Provider Switching

Implement real LLM calls in the AI API with a swappable provider layer and first-class mocking. Support Google and OpenRouter providers with three model tiers: chat (normal), interaction classification (ultralight), and assessment (heavier). Land thorough tests, monitoring, and docs while preserving existing flows.

## Meta
- Sprint ID: SPR-005
- Status: active
- Start date: 23/08/2025
- End date:
- Links: [Overview](./overview.md) · [PRD](../../planning/prd.md) · [Technical Overview](../../planning/technical-overview.md) · [Monitoring](../../ops/monitoring.md) · [Benchmarking](../../ops/benchmarking.md) · [Features CSV](../features.csv) · [AI API Reference](../../../../coach-up-ai-api/docs/api/ai/reference.md)

## Progress — 24/08/2025
* **Classifier provider integration (AI API)**
  - Gated via `AI_CLASSIFIER_ENABLED`; `/messages/ingest` now calls provider through `get_classifier_client()`.
  - Fallback chain: provider -> mock classifier -> local stub; heuristic override on low confidence maintained. Thresholds preserved (`CLASSIFIER_CONF_ACCEPT`, `CLASSIFIER_CONF_LOW`).
  - Event logging enriched with `classifierEnabled`, `classifierProvider`, `classifierModel`.
* **Provider clients scaffolding**
  - Added placeholder `GoogleClassifierClient`/`GoogleAssessClient` (scaffolds; raise `NotImplementedError`, enforce API key presence) to unblock factory wiring.
  - Implemented `OpenRouterChatClient`/`OpenRouterClassifierClient`/`OpenRouterAssessClient` with real HTTP/SSE using `httpx`.
  - Extended `mock.py` with `MockClassifierClient` and `MockAssessClient` (deterministic behavior kept for CI).
* **Factory**
  - `app/providers/factory.py` now exports `get_classifier_client()` and `get_assess_client()` with per-role precedence and robust fallbacks to mock.
* **Environment**
  - Updated `.env.example` with `AI_CLASSIFIER_ENABLED`, `AI_CLASSIFIER_MODEL`, `AI_ASSESS_ENABLED`, `AI_ASSESS_MODEL`; clarified legacy `DISABLE_CLASSIFIER` override.
* **Sanity**
  - Import check for `app.main` passes; API app loads successfully.

## Progress — 25/08/2025
* **Kickoff**
  - Sprint started; documenting scope, defaults, and observability metrics. No code changes claimed.
  - Confirmed conventions: UK dates; End date left blank while active; 'Documentation' section present.
  - Plan: mock providers remain default in CI; real providers used only for manual verification.

* **Decisions/Docs**
  - Default model IDs selected for Google and OpenRouter (see Configuration & flags → Default models). No code changes yet; values to be used when enabling real providers.
  - Testing plan confirmed: CI uses deterministic mock provider by default; manual real-provider runs via env toggles only. E2E retains mock by default.

## Progress — 25/08/2025 (update)
* **Chat streaming via OpenRouter (AI API)**
  - Enabled end-to-end with flags `AI_CHAT_ENABLED=1`, `AI_PROVIDER_CHAT=openrouter`, model default `openai/gpt-4o-mini`.
  - `/chat/stream` now streams real tokens from `OpenRouterChatClient.stream_chat()`; verified TTFT and completion.
  - Structured logs emitted: `chat_stream_first_token` (includes `provider`, `model`, `ttft_ms`) and `chat_stream_complete`.
  - Added error logging: `chat_stream_provider_error` with `requestId`, `provider`, `model`, and exception; falls back to stub stream when provider errors.

* **Assessments via OpenRouter (AI API)**
  - `OpenRouterAssessClient.assess()` integrated; `/messages/ingest` enqueues on interaction end; `/assessments/{sessionId}` returns content-specific summaries.
  - Deterministic fallback preserved on errors. Persistence endpoint returned 401 locally (expected while not configured); assessment summary retrieval still works.

* **Boundary classifier (LLM) behind flag**
  - `AI_CLASSIFIER_ENABLED=1` routes through `OpenRouterClassifierClient.classify()`; heuristic fallback retained with thresholds.

* **Providers/factory**
  - `app/providers/openrouter.py` implemented using `httpx` and SSE parsing. Factory selects provider via `AI_PROVIDER_*` overrides.

* **Observability**
  - User-Agent set; request timeouts added. Structured logs include `requestId`, `provider`, `model`, `ttft_ms`, `total_ms`.
  - Prometheus metrics implemented in AI API: HTTP request totals/durations; chat TTFT/total by provider/model; assessment queue depth and job timings; optional SQS send/receive/delete timings.

## Progress — 25/08/2025 (verification)
* **Endpoints present (AI API)**
  - `/chat/stream` (GET, SSE) — real provider streaming when `AI_CHAT_ENABLED=1`; stub fallback on errors.
  - `/messages/ingest` (POST) — ingests messages, runs LLM boundary classification when `AI_CLASSIFIER_ENABLED=1`, updates session/group state, enqueues assessment on interaction end.
  - `/assessments/run` (POST) — enqueues a background assessment job and returns `groupId`.
  - `/assessments/{sessionId}` (GET) — returns latest assessment summary for a session; if missing, computes on-demand from last known `groupId`.
  - `/service-metrics` (GET) — lightweight service metrics: `queueDepth`, `workerConcurrency`, `sessionCount`, `resultsCount`.
  - `/health` (GET) — liveness.
  - `/metrics` (GET) — Prometheus exposition format (503 if Prometheus client unavailable).
  - Source: `coach-up-ai-api/app/main.py`.
* **Assessment job**
  - `_run_assessment_job()` slices in-memory transcript by `(sessionId, groupId)` span and, when `AI_ASSESS_ENABLED=1`, calls `get_assess_client().assess(...)` with fallback to deterministic stub. Emits `assessments_job_start/complete` logs and histograms.
  - Output retains rubric v1 category scores + lightweight summary, and now includes per-skill fan-out with `skillAssessments[]` populated when tracked skills are present; partial failures are logged and counted via Prometheus.
* **Workers & queue**
  - Lifespan spawns N workers (`WORKER_CONCURRENCY`). In-memory queue by default; optional SQS path guarded by `USE_SQS` with metrics for send/receive/delete/visibility. `coachup_assessments_queue_depth` gauge exported. 
* **Docs/Monitoring**
  - Updated `docs/ops/monitoring.md` assessment worker examples to include `trackedSkillIdHash` on: `assessments_dequeue`, `assessments_retry`, `assessments_job_start`, `assessments_scores`, `assessments_job_complete`.
  - Clarified provider-specific fields: in-memory `assessments_dequeue` includes `queueDepth` and may include `enqueueLatencyMs`; SQS variant includes `provider: "sqs"` and `enqueueLatencyMs`.

## Progress — 25/08/2025 (tests)
* **Skill-aligned assessments (tests & persistence)**
  - Added tests for concurrent per-skill fan-out, aggregation correctness, partial failure handling with error metrics, latency histogram, and persistence including `summary.skillAssessments`.
  - Added provider test asserting `OpenRouterAssessClient` sets `X-Tracked-Skill-Id` and `X-Tracked-Skill-Id-Hash` when a skill is provided.
  - Full suite green locally (41 tests). Per-skill metrics observed: `coachup_assessment_skill_seconds` and `coachup_assessment_skill_errors_total`.

## Progress — 25/08/2025 (migration)
* **Assessment persistence v2-only (AI API)**
  - Removed legacy v1 persistence branch and `ASSESS_OUTPUT_V2` flag from `coach-up-ai-api/app/main.py`.
  - `_persist_assessment_if_configured()` now always posts `rubricVersion: "v2"` with the v2 skill-aligned payload built by `_build_v2_persist_payload()`.
  - Updated `coach-up-ai-api/.env.example` to remove `ASSESS_OUTPUT_V2` and document v2-only persistence. 
  - Test suite green (42 tests) after migration.
 
## Progress — 26/08/2025 (tests)
* **Tests migrated to v2-only (AI API)**
  - Updated assessment-related tests to use `rubricVersion: "v2"` and removed deprecated `ASSESS_OUTPUT_V2` usage in tests.
  - Files: `tests/test_provider_skill_headers.py`, `tests/test_skill_assessments.py`, `tests/test_assessments.py`, `tests/test_request_id_and_metrics.py`, `tests/test_persist_v2.py`.
  - Full test suite passing: 42/42.
 
## Progress — 26/08/2025 (observability & docs)
* **Observability (AI API & Infra)**
  - Added Prometheus counters `coachup_assessment_tokens_total` and `coachup_assessment_cost_usd_total` labeled by provider/model/rubric.
  - Grafana dashboard updated: panels for Tokens/min (in/out by provider/model) and Cost USD/min by provider/model.
  - Prometheus alerts added: token rate warn/page and cost rate warn/page.
* **Docs/OpenAPI**
  - `docs/api/ai/reference.md`: added metrics section (tokens/cost); corrected job outcome metric; noted histogram p95 usage.
  - Regenerated OpenAPI snapshot and linted (Spectral) with no errors.
* **Verification**
  - Backend tests passing (42/42). Provider/model switching verified; timeouts documented in API reference.

## Progress — 26/08/2025 (frontend & UI API)
* **Skills pages (Frontend)**
  - Implemented `ui/src/app/skills/page.tsx` and `ui/src/app/skills/[id]/page.tsx` to list skills, manage tracked skills (max 2), and show per-skill criteria and assessments.
  - Detail page sends `X-Tracked-Skill-Id` to `/api/assessments/{sessionId}` to filter v2 `skillAssessments`.
  - Gap: detail page tracking does not pass `order`; UI control to set `currentLevel` is pending.
* **UI API routes (Next.js)**
  - Added `ui/src/app/api/v1/skills/*` endpoints for list, tracked, track, untrack, and level with mock vs Convex modes, CORS, and Prometheus metrics.
  - Added `ui/src/app/api/assessments/[sessionId]/route.ts` proxy that forwards `X-Tracked-Skill-Id` to AI API. Gap: CSV `X-Tracked-Skill-Ids` pending.

## Objectives (Tick when achieved) 
- [x] Real LLM provider abstraction with Google and OpenRouter backends, plus mock
- [x] Chat SSE streams tokens from real provider model (fallback to mock)
- [x] Interaction classification calls ultralight model with heuristic fallback
- [x] Real LLM calls are used for chat, classifier, and assessor when enabled (mock remains default in CI)
- [x] Assessment job outputs simple, skill-aligned results (per tracked skill: level + met/unmet criteria + brief feedback) — no rubric v1
- [x] Provider/model switching via env flags without code changes
- [ ] Tests: unit, integration, e2e green; deterministic mock for CI
- [x] Metrics/logging include provider, modelId, tokens, cost, latency; dashboards updated
 - [x] API docs and examples updated; OpenAPI generated and linted

## Planned Tasks
- [x] Add provider abstraction in AI API — owner: backend (<5d)
  - [x] Create `app/providers/base.py` with `ChatClient`, `ClassifierClient`, `AssessClient` interfaces
  - [x] Implement `app/providers/google.py` (scaffolded clients) and `app/providers/openrouter.py` (scaffolded clients)
  - [x] Implement `app/providers/mock.py` (deterministic mock; classifier/assess added)
  - [x] Add `app/providers/factory.py` to instantiate based on env (`AI_PROVIDER`, per-role overrides)
  - [x] Add User-Agent headers and request timeouts to provider clients
  - [x] Propagate `requestId` to providers via headers (tests in `coach-up-ai-api/tests/test_request_id_and_metrics.py`)

- [x] Chat SSE uses real model — owner: backend (<2d)
  - [x] Use provider stream when `AI_CHAT_ENABLED=1` (keeps stub fallback on errors)
  - [x] Emit TTFT and provider/model in structured logs
  - [x] Export Prometheus metrics with provider/model labels for TTFT and duration

- [x] Interaction classification LLM — owner: backend (<2d)
  - [x] Replace `_classify_message_llm_stub` with provider call when `AI_CLASSIFIER_ENABLED=1`
  - [x] Keep heuristic fallback for low/abstain and errors; preserve thresholds
  - [x] Log decision, confidence, provider/model in event payload (prompts/schema TBD)

- [ ] Assessment job LLM — owner: backend (<3d)
  - [x] In `_run_assessment_job`, when enabled, call assess provider to compute per-skill level and feedback from transcript slice 
  - [x] Preserve deterministic mock outputs to keep CI stable (no dependency on real LLMs)
  - [ ] Include transcript slice context and tracked skill criteria in prompt; cap tokens/latency
  - [x] Fan-out: spawn one assessor LLM call per tracked skill for each queued interaction; pass the skill definition and criteria to the prompt
  - [x] Aggregation: build one group result with `skillAssessments[]` in stable order; aggregate tokens/cost; include per-skill errors for partial failures; enforce per-call timeouts and a group deadline

- [x] Assessment model migration  — owner: backend (<1d)
  - [x] Deprecate rubric v1 in persistence; emit v2 skill-aligned payload only:
        `{ skillAssessments: [{ skillHash, level, metCriteria[], unmetCriteria[], feedback[] }], meta{ provider, model, skillsCount } }`
  - [x] Implement v2 adapter in `coach-up-ai-api/app/main.py` to map results to `{skillAssessments[], meta{}}` and always send `rubricVersion: "v2"`
  - [x] Update `.env.example` and docs to remove `ASSESS_OUTPUT_V2`; add migration notes for consumers
  - [x] Persistence contract verified in tests (`tests/test_persist_v2.py`, `tests/test_persist.py`)
  - [x] Update AI API reference (`coach-up-ai-api/docs/api/ai/reference.md`) with v2 examples and deprecation note for v1
  - [x] Inventory and adjust any rubric-related tests/goldens still expecting v1 shapes (if any)

- [ ] Tracked skills integration — owner: backend (<2d)
  - [ ] Fetch tracked skills for the session (or from `x-tracked-skill-id(s)`) via Core/UI API or Convex
  - [ ] Env: `SKILLS_API_URL`, `SKILLS_API_SECRET` (service-to-service); support `X-Request-Id` propagation
  - [ ] Support multiple skills; pass skill definitions to assessor; log `skillCount` and hashed IDs
  - [x] Backwards-compatible path when no skills present (assess general communication)
  - [ ] Implement `skills_client` with retry/backoff and request-id headers; provider switch `SKILLS_PROVIDER` (convex|core)
  - [ ] Accept both `x-tracked-skill-id` and `x-tracked-skill-ids` (CSV) on ingress; validate/normalize (single header supported end-to-end; CSV pending)
  - [x] Hashing: compute `skillHash` with SHA-256; consider optional salt `SKILL_HASH_SALT` (documented) — implemented in worker logs as `trackedSkillIdHash`; salt documentation follow-up pending
  - [ ] Wire fetched skills into `_run_assessment_job` fan-out; include criteria in prompts; cap tokens/latency
  - [ ] Metrics/logs: emit `skillCount`, list of `skillHash` (not raw IDs), provider/model labels
  - [ ] Tests: unit (client + headers), integration (fetch + fan-out), error paths (timeouts/empty)

- [ ] Configuration & flags — owner: backend (<1d)
  - [x] Env vars: `AI_PROVIDER`, `AI_CHAT_MODEL`, `AI_CLASSIFIER_MODEL`, `AI_ASSESS_MODEL`
  - [x] Provider keys: `GOOGLE_API_KEY`, `OPENROUTER_API_KEY`
  - [x] Per-role provider overrides: `AI_PROVIDER_CHAT`, `AI_PROVIDER_CLASSIFIER`, `AI_PROVIDER_ASSESS`
  - [x] Feature flags: `AI_CHAT_ENABLED`, `AI_CLASSIFIER_ENABLED`, `AI_ASSESS_ENABLED`; default off for CIcript or uvicorn invocation) and identify the endpoints for ingesting messages and assessments so I can craft smoke test commands accurately. Then I'll propose safe, non-destructive commands to s
  - [x] Update `.env.example` (API docs update pending)
  - [x] Default models:
    - Google: chat=`gemini-2.5-flash`, classifier=`gemini-2.5-flash-lite`, assess=`gemini-2.5-flash`
    - OpenRouter: 
      chat=`google/gemini-2.5-flash`, 
      classifier=`google/gemini-2.5-flash-lite`, 
      assess=`google/gemini-2.5-flash`
  - [ ] Skills fetch config: `SKILLS_API_URL`, `SKILLS_API_SECRET`, optional `SKILLS_PROVIDER` (convex|core)

- [ ] Observability — owner: infra (<2d)
  - [x] Add Prometheus counters/histograms: tokens, cost_usd, ttft_seconds, duration_seconds by provider/model/route
  - [x] Log fields: `requestId`, `provider`, `modelId`, `tokensPrompt`, `tokensOutput`, `costUsd`, `latencyMs`, `trackedSkillIdHash`; per-skill logs include `skillId`, `skillHash`, and `latency_ms`/`error`
  - [x] Update Grafana dashboards: chat SSE, assessments latency, queue depth, provider/model breakdown
  - [x] Define alert rules and thresholds (non-enforcing cost alerts; enforcement tracked in FEAT-022):
    - chat_ttft_p95 > 1.2s for 5m (warn); > 1.5s for 10m (crit)
    - assessment_completion_p95 > 8s for 10m (warn); > 12s for 10m (crit)
    - http_5xx_rate > 1% for 5m (warn); > 5% for 5m (crit)
    - queue_depth > 100 for 10m (warn); > 500 for 10m (crit)
    - cost_usd_daily > 80% of `COST_BUDGET_USD_DAILY` (warn); > 100% (crit)
  - [ ] Alert routing configured (Slack channel + on-call escalation for critical alerts)

- [ ] Testing — owner: QA (<3d)
  - [ ] Unit tests: provider factory selection, env parsing, chat stream adapter, classifier fallback, assess mapping
  - [ ] Integration tests: chat SSE happy path (mock provider), classifier decisions, assessment end-to-end (mock)
  - [ ] Contract tests: request/response shapes unchanged on `/chat/stream`, `/messages/ingest`, `/assessments/*`
  - [ ] E2E (Playwright) smoke uses mock provider; add toggle for real provider manual runs
  - [ ] Load/SSE k6 smoke for chat stream

- [ ] Documentation & examples — owner: docs (<1d)
  - [x] Update `docs/api/ai/reference.md` with provider/model switching and headers
  - [x] Regenerate `/openapi.json`; add example curl + TS for both providers
  - [ ] Add `examples/request_id_middleware.py` mention for request tracing

## Scope
In scope
- Real LLM integration for chat, classification, and assessment via provider layer
- Provider switching and mocking via env flags
- Metrics/logging and dashboards for new paths
- Tests (unit, integration, e2e) and API docs updates

Out of scope
- Auth (JWT verification) beyond current stubs
- Cost alarms/budget enforcement (tracked in FEAT-022)
- STT/TTS integrations

## Features in this Sprint
 - [ ] FEAT-004 — Provider Abstraction Layer (ai)
 - [ ] FEAT-011 — Multi-turn Assessment Job v1 (ai)
 - [ ] FEAT-019 — Monitoring Baseline (infra)
 - [ ] FEAT-028 — Skill-aligned Assessments v0 (ai)
 - [ ] FEAT-030 — LLM Boundary Classifier (ai)

## Acceptance Criteria
- [x] `AI_PROVIDER` and per-role overrides select Google, OpenRouter, or mock without code changes
- [x] `/chat/stream` streams tokens from the configured chat model; TTFT logged and < 1.2s p95 with mock
- [x] `/messages/ingest` uses LLM classification when enabled; retains heuristic fallback; decisions logged with confidence
- [x] `/assessments/run` + `/assessments/{sessionId}` produce simple skill-aligned results (per tracked skill: level + met/unmet criteria + brief feedback); heavy model path guarded by flag
  - [x] For N tracked skills in an interaction, N assessor LLM calls are executed and N items are returned in `skillAssessments[]`
 - [x] Aggregation returns a single group payload with stable ordering; partial failures are included as per-skill errors; `meta` aggregates tokens/cost and latency
 - [ ] Assessor fetches tracked skills and evaluates against them; summarizer outputs highlights/recommendations; orchestrator composes
 - [x] Metrics exported with provider/model labels; dashboards updated and linked
 - [ ] Tests: all unit/integration/e2e pass with mock provider in CI; deterministic goldens preserved (updated to new output shape)
 - [x] Docs updated; OpenAPI refreshed; examples added
- [x] Manual local verification with Google and OpenRouter providers using real keys

## Risks & Mitigations
- Risk: Provider quotas/latency spikes · Mitigation: default mock in CI; timeouts/retries; provider failover
- Risk: Non-deterministic tests due to real LLMs · Mitigation: mock by default; snapshot tests only on mock
- Risk: SSE proxy timeouts · Mitigation: heartbeat every 15s; backpressure handling; timeouts
- Risk: Cost overruns · Mitigation: cost metrics per request; manual toggle for real-provider E2E only

## Dependencies
- Depends on features/sprints: FEAT-019, FEAT-028, FEAT-011
- External: Google Generative AI API key, OpenRouter API key; optional SQS for assessments

## Technical Details
### Database Models
- Collections/Tables impacted: none (in-memory + optional HTTP persist remains)
- New/changed fields: N/A
- Constraints/validation: N/A
- Indexes (read/write paths): N/A
- Migrations: N/A — Backfill plan: N/A
- Data retention: raw logs 30d; SSE logs sampled

### Algorithmic Details
- Approach: LLM-first with heuristic fallback (minimal-edit); deterministic mock for CI
- Skill-aligned output: v0 (per tracked skill: assign level and identify met/unmet criteria; produce brief feedback)
- Fan-out: for each queued interaction with N tracked skills, spawn N independent assessor LLM calls; each call receives the specific skill definition
- Aggregation: collect per-skill results into `skillAssessments[]` with stable ordering; aggregate tokens/cost and latency; include per-skill errors on timeouts/failures; enforce per-call timeouts and a group deadline
- Latency targets: TTFT < 1.2s (mock); full-turn < 2.5s (mock)

### Prompts & Scoring
- Prompt names/links: chat.system, classifier.boundary, assess.skill-v0
- Leveling criteria: per tracked skill; identify met/unmet criteria and provide brief feedback

## QA & Testing
 - [ ] Unit tests updated/added (providers, factory, adapters)
 - [ ] E2E happy path green (Playwright) with mock provider
 - [ ] Assert N tracked skills -> N assessor invocations and mapping to `skillAssessments[]`; test partial failure path (timeouts/errors)
 - [ ] Load/SSE smoke test passes (k6)
 - [ ] Contract tests for API routes (`/chat/stream`, `/messages/ingest`, `/assessments/*`)

## Observability & SLOs
Targets (see Technical Overview §10)
- Realtime chat p95 TTFT < 1.2s (mock); full-turn < 2.5s (mock)
- Assessment p95 completion < 8s (mock/in-memory)
- Alert thresholds
  - Chat TTFT p95 > 1.2s 5m warn; > 1.5s 10m crit
  - Assessment completion p95 > 8s 10m warn; > 12s 10m crit
  - HTTP 5xx rate > 1% 5m warn; > 5% 5m crit
  - Queue depth > 100 10m warn; > 500 10m crit
  - Daily cost spend > 80% budget warn; > 100% budget crit (uses `COST_BUDGET_USD_DAILY`)

Checkpoints
- [x] Dashboards exist/updated (links)
- [x] Alerts configured (links)
- [x] Structured logs include `requestId`, `provider`, `modelId`, tokens, cost, latency, and `trackedSkillIdHash`; per-skill events (`assessment_skill_complete`/`assessment_skill_error`) include `skillId`/`skillHash`
- [x] Alert thresholds documented and rules enabled in Grafana/Prometheus
- [x] Ops runbook linked (monitoring, alert response, dashboards)
- [x] On-call rotation documented with escalation path

## Operational Hygiene
- [x] CI checks green (API Docs workflow, tests/linting)
- [x] Branch protection respected (PR + review)
- [x] Pre-commit hooks executed (lint/format/type checks)
- [x] .env.example updated if new env vars added
- [x] Request ID propagated end-to-end for changed paths
- [x] Logs include: requestId, route, userId (if available), provider, modelId, tokens, cost, latency, trackedSkillIdHash; per-skill logs include skillId/skillHash and latency_ms/error
- [x] Rate limiting and idempotency considered for new/changed endpoints
- [x] On-call schedule maintained; escalation policy verified
- [x] Ops runbook created (monitoring, alert thresholds, troubleshooting) and referenced
- [x] Incident template used; postmortems recorded and linked

## Documentation
- [x] API reference updated for endpoints touched (Core & AI)
- [x] OpenAPI spec updated and linted
- [x] Examples added/verified (curl + TypeScript)
- [x] Cross-links updated (PRD/Technical Overview)
- [x] Ops runbook added and linked (dashboards, alerts, playbooks)

## Post-sprint
- [ ] KPIs reviewed; compare to targets
- [ ] Retrospective completed; action items filed
- [ ] Docs updated (PRD/Tech Overview/Runbooks)

## Change Log
- 23/08/2025 Created sprint page
- 24/08/2025 Conventions applied: UK date format; End date cleared while active
- 24/08/2025 Classifier provider integration gated by `AI_CLASSIFIER_ENABLED`; provider scaffolds added; factory extended; `.env.example` updated; ingest logs enriched
- 25/08/2025 Kickoff: added docs-only progress section; clarified defaults/observability; no code changes
- 25/08/2025 Decisions: selected default model IDs for Google/OpenRouter; documented in sprint (no code changes)
- 25/08/2025 Implemented per-skill fan-out in assessments with `skillAssessments[]`, emitted per-skill metrics, persisted `skillAssessments` in finalize payload; added tests (fan-out, aggregation, partial failure, latency, provider skill headers)
- 25/08/2025 Testing plan: CI stays on deterministic mock; real-provider runs only via env toggles/manual verification
- 25/08/2025 Monitoring docs: aligned assessment worker log examples with implementation (added `trackedSkillIdHash`; clarified `enqueueLatencyMs` and provider-specific fields)
- 25/08/2025 Assessment persistence migrated to v2-only in AI API; removed `ASSESS_OUTPUT_V2`; `.env.example` updated; tests green (42)
- 26/08/2025 API reference updated: documented v2-only persistence payload and removal of `ASSESS_OUTPUT_V2`; added persistence section with example
- 26/08/2025 Tests updated to v2-only: migrated assessment tests to `rubricVersion: "v2"`, removed `ASSESS_OUTPUT_V2` usage in tests; all AI API tests passing (42/42)
 - 26/08/2025 Observability: added tokens/cost counters; updated Grafana panels (Tokens/min, Cost USD/min); added Prometheus alerts for token/cost rates
 - 26/08/2025 Frontend & UI API: implemented skills list/detail pages and skills API routes in UI; assessments proxy forwards single `X-Tracked-Skill-Id`. Noted gaps: detail page track order, level control UI, and CSV `X-Tracked-Skill-Ids` support
 - 26/08/2025 API docs & OpenAPI: documented new metrics (tokens/cost) in AI API reference; regenerated and linted OpenAPI snapshot